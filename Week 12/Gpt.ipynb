{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building GPT \n",
    "\n",
    "Based on: \n",
    "1) https://github.com/karpathy/nanoGPT\n",
    "2) https://youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be implementing 4 types of self-attention implementations\n",
    "1) Barely an implementation of self-attention, averaging past context\n",
    "2) implementation with matrix multiplication\n",
    "3) adding softmax on top\n",
    "    * Additionally, we implement variable embedding sizes and incorporating positional encoding\n",
    "4) the real self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "Unique Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab Size: 65\n"
     ]
    }
   ],
   "source": [
    "# read it in to inspect it\n",
    "with open('./Shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Unique Characters: \" + ''.join(chars))\n",
    "print(\"Vocab Size: \" + str(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is generally a trade off between Vocab Size and Sequence lengths, our vocabulary size is tiny since it is a character level vocabulary, but sequences turn out very large. There are many methods that bring out a better balance between the vocabulary and sequence lengths, but for simplicity we will keep the scheme that we have been using. \n",
    "\n",
    "in practice, each character in our set up is a token, thus they will be referred to as tokens from hereon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([ 0, 18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
      "        44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52,\n",
      "        63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,\n",
      "         1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39,\n",
      "        49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15,\n",
      "        47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50,\n",
      "        50,  1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1,\n",
      "        58, 53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51,\n",
      "        47, 57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43,\n",
      "        42,  8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1,\n",
      "        63, 53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56,\n",
      "        41, 47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51,\n",
      "        63,  1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0,\n",
      "        13, 50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,\n",
      "         1, 49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,\n",
      "         1, 46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39,\n",
      "        60, 43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,\n",
      "         1, 54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56,\n",
      "        42, 47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56,\n",
      "        43,  1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43,\n",
      "        58,  1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,\n",
      "         6,  1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45,\n",
      "        53, 53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56,\n",
      "        57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,\n",
      "         1, 39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47,\n",
      "        58, 47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41,\n",
      "        47, 39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59,\n",
      "        58, 46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53,\n",
      "        52,  1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57,\n",
      "        10,  1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47,\n",
      "        43, 50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54,\n",
      "        43, 56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,\n",
      "         1, 61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61,\n",
      "        43,  1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,\n",
      "         1, 56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52,\n",
      "        43, 50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52,\n",
      "        49,  1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,\n",
      "         1, 58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,\n",
      "         0, 39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1,\n",
      "        53, 40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43,\n",
      "        56, 63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52,\n",
      "        58, 53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56,\n",
      "        47, 57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41,\n",
      "        43, 11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1,\n",
      "        47, 57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1,\n",
      "        24, 43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47,\n",
      "        57,  1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1,\n",
      "        43, 56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43,\n",
      "        57, 10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52,\n",
      "        53, 61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,\n",
      "         1, 46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,\n",
      "         1, 52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,\n",
      "         1, 56, 43, 60, 43, 52, 45, 43,  8,  0])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input tensor is [0], target is 18\n",
      "when input tensor is [0, 18], target is 47\n",
      "when input tensor is [0, 18, 47], target is 56\n",
      "when input tensor is [0, 18, 47, 56], target is 57\n",
      "when input tensor is [0, 18, 47, 56, 57], target is 58\n",
      "when input tensor is [0, 18, 47, 56, 57, 58], target is 1\n",
      "when input tensor is [0, 18, 47, 56, 57, 58, 1], target is 15\n",
      "when input tensor is [0, 18, 47, 56, 57, 58, 1, 15], target is 47\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# And dataloaders \n",
    "\n",
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input tensor is {context.tolist()}, target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice that we train on not only sequences of length n, but all the x <= n sequences. This is done, not only for computations reasons, but also to allow the model to learn how to make predictions on smaller sequences than n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "shapetorch.Size([4, 8])\n",
      "data: tensor([[ 0, 24, 43, 58,  5, 57,  1, 46],\n",
      "        [ 1, 44, 53, 56,  1, 58, 46, 39],\n",
      "        [43, 52, 58,  1, 58, 46, 39, 58],\n",
      "        [27, 25, 17, 27, 10,  0, 21,  1]])\n",
      "targets:\n",
      "shapetorch.Size([4, 8])\n",
      "data: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "# number of input_examples = batch_size * block_size (4 * 8 = 32)\n",
    "\n",
    "def get_batch(split):\n",
    "    # Select the appropriate dataset based on the split parameter\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "\n",
    "    # Generate a batch of random starting indices within the dataset\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # Select a block of text of size block_size starting from each random index\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "\n",
    "    # Shift the selected block of text by one character to the right to create the target sequence\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(f\"inputs:\\nshape{xb.shape}\\ndata: {xb}\")\n",
    "print(f\"targets:\\nshape{yb.shape}\\ndata: {yb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by quickly building a bigram model and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([32, 65])\n",
      "loss: 4.7389655113220215 | we are expecting a loss of around 4.174387454986572\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a loockup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # idx and targets are both (B, T) tensor of ints\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) = (4, 8 , vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # note that F.cross_entropy accepts inputs in shape (B, C, T)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T) # can be as targets = targets.view(-1)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the logits for the next token\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            # (note that we are feeding the whole context each time, however we only care about the last prediction)\n",
    "            # (this make doesn't make sense now, but the function will be modified later)\n",
    "            logits = logits[:, -1, :] # Becomes (B, C) (get the last time step for each sequence)\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim = -1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled token to the context\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
    "        return idx\n",
    "    \n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(f\"logits shape: {logits.shape}\")\n",
    "print(f\"loss: {loss} | we are expecting a loss of around {torch.log(torch.tensor(vocab_size))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype = torch.long)\n",
    "generated = m.generate(idx, 100) # shape (1, 101)\n",
    "print(decode(generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.5532305240631104\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(10000):\n",
    "    # sample a batch of training data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "print(f\"loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e2cb720d60>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henouratucenonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZAnde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scar t tridesar, wnl'shenou\n"
     ]
    }
   ],
   "source": [
    "# sampling from the model\n",
    "idx = torch.zeros((1,1), dtype = torch.long)\n",
    "generated = m.generate(idx, 400) # shape (1, 101)\n",
    "print(decode(generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can just as easily, port the bigram model to a .py file and summon it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python bigram.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 1: Averaging Previous Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the next token accurately, it's very useful if each token considers the context provided by all its predecessors. The self-attention mechanism facilitates this by allowing each token to interact directly with every previous token. One straightforward method to implement this is by averaging the embeddings of prior tokens. This approach simplifies the communication between tokens but comes with a tradeoffâ€”it can lead to a loss of detailed spatial information, as the specific positions of tokens within the sequence are not clear. This method is effective for capturing general context, but it will overlook the exact order of tokens, leading to suboptimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch size, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "print(f\"x shape: {x.shape}\")\n",
    "\n",
    "# We want x[b, t] = mean_(i<=t) x[b, i]\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, dim = 0) # average over time dimension (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0]: tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "xbow[0]: tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# Let's Check the first Batch\n",
    "print(f\"x[0]: {x[0]}\")\n",
    "print(f\"xbow[0]: {xbow[0]}\")\n",
    "\n",
    "# the first row is the same \n",
    "print(x[0, 0] == xbow[0, 0])\n",
    "# the second row is the average of the first two rows\n",
    "print((x[0, 0] + x[0, 1]) / 2 == xbow[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 2: Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enhance efficiency, we can replace nested loops with matrix multiplication. This is achieved by multiplying the data matrix by a lower triangular matrix. This approach leverages the properties of linear algebra to ensure that each token is influenced only by preceding tokens, effectively maintaining the sequence's order while streamlining the computational process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (shape = torch.Size([3, 3])) =\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b (shape = torch.Size([3, 2])) =\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c (shape = torch.Size([3, 2])) =\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# lower triangular matrix of ones\n",
    "a = torch.tril(torch.ones(3,3)) \n",
    "# make all rows sum to 1\n",
    "a = a / torch.sum(a, 1, keepdim = True)\n",
    "# create a random matrix\n",
    "b = torch.randint(0, 10, (3, 2)).float() \n",
    "\n",
    "c = a @ b\n",
    "print(f\"a (shape = {a.shape}) =\\n{a}\")\n",
    "print(f\"b (shape = {b.shape}) =\\n{b}\")\n",
    "print(f\"c (shape = {c.shape}) =\\n{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      " step 0 | train loss: 4.4090 | val loss: 4.4038\n",
      " step 100 | train loss: 2.6517 | val loss: 2.6604\n",
      " step 200 | train loss: 2.5039 | val loss: 2.5026\n",
      " step 300 | train loss: 2.4024 | val loss: 2.4111\n",
      " step 400 | train loss: 2.3420 | val loss: 2.3542\n",
      " step 500 | train loss: 2.2987 | val loss: 2.3059\n",
      " step 600 | train loss: 2.2390 | val loss: 2.2619\n",
      " step 700 | train loss: 2.2038 | val loss: 2.2211\n",
      " step 800 | train loss: 2.1629 | val loss: 2.1864\n",
      " step 900 | train loss: 2.1256 | val loss: 2.1533\n",
      " step 1000 | train loss: 2.0873 | val loss: 2.1280\n",
      " step 1100 | train loss: 2.0720 | val loss: 2.1124\n",
      " step 1200 | train loss: 2.0420 | val loss: 2.0878\n",
      " step 1300 | train loss: 2.0200 | val loss: 2.0635\n",
      " step 1400 | train loss: 2.0102 | val loss: 2.0633\n",
      " step 1500 | train loss: 1.9883 | val loss: 2.0434\n",
      " step 1600 | train loss: 1.9615 | val loss: 2.0379\n",
      " step 1700 | train loss: 1.9424 | val loss: 2.0214\n",
      " step 1800 | train loss: 1.9238 | val loss: 2.0167\n",
      " step 1900 | train loss: 1.9206 | val loss: 2.0138\n",
      " step 2000 | train loss: 1.8881 | val loss: 1.9906\n",
      " step 2100 | train loss: 1.8719 | val loss: 1.9777\n",
      " step 2200 | train loss: 1.8680 | val loss: 1.9663\n",
      " step 2300 | train loss: 1.8526 | val loss: 1.9518\n",
      " step 2400 | train loss: 1.8477 | val loss: 1.9597\n",
      " step 2500 | train loss: 1.8366 | val loss: 1.9547\n",
      " step 2600 | train loss: 1.8164 | val loss: 1.9439\n",
      " step 2700 | train loss: 1.8223 | val loss: 1.9425\n",
      " step 2800 | train loss: 1.8026 | val loss: 1.9231\n",
      " step 2900 | train loss: 1.7932 | val loss: 1.9122\n",
      " step 3000 | train loss: 1.7853 | val loss: 1.9066\n",
      " step 3100 | train loss: 1.7822 | val loss: 1.9191\n",
      " step 3200 | train loss: 1.7788 | val loss: 1.9232\n",
      " step 3300 | train loss: 1.7611 | val loss: 1.9015\n",
      " step 3400 | train loss: 1.7622 | val loss: 1.9001\n",
      " step 3500 | train loss: 1.7485 | val loss: 1.8972\n",
      " step 3600 | train loss: 1.7404 | val loss: 1.8746\n",
      " step 3700 | train loss: 1.7339 | val loss: 1.8874\n",
      " step 3800 | train loss: 1.7229 | val loss: 1.8834\n",
      " step 3900 | train loss: 1.7343 | val loss: 1.8763\n",
      " step 4000 | train loss: 1.7225 | val loss: 1.8804\n",
      " step 4100 | train loss: 1.7148 | val loss: 1.8690\n",
      " step 4200 | train loss: 1.7173 | val loss: 1.8654\n",
      " step 4300 | train loss: 1.7099 | val loss: 1.8605\n",
      " step 4400 | train loss: 1.6982 | val loss: 1.8506\n",
      " step 4500 | train loss: 1.7058 | val loss: 1.8603\n",
      " step 4600 | train loss: 1.6990 | val loss: 1.8557\n",
      " step 4700 | train loss: 1.6925 | val loss: 1.8627\n",
      " step 4800 | train loss: 1.6892 | val loss: 1.8454\n",
      " step 4900 | train loss: 1.6732 | val loss: 1.8260\n",
      "\n",
      "ift, mudging have again and alren\n",
      "There to sweecr'd too fore atear fring\n",
      "As as with thone betted enenter the part\n",
      "Word have I am mow I have ungent,\n",
      "Mumbraving to itchmon, Sable\n",
      "Y be love, and will not I itsunteed!\n",
      "I cailing pifided's to must, that for rose repoln;\n",
      "I proy and much thing ash, whose havself with lave infected to refended old.\n",
      "\n",
      "PRINCHORD:\n",
      "Come, upon dedisun all the must good and them liver!\n",
      "\n",
      "HENRY VORAK:\n",
      "Gresune's sidle it-kill ply live?\n",
      "\n",
      "Nurse:\n",
      "I than ougety, this plovost mujor\n",
      "bri\n"
     ]
    }
   ],
   "source": [
    "# We want x[b, t] = mean_(i<=t) x[b, i]\n",
    "wei = torch.tril(torch.ones(T, T)) # (T, T)\n",
    "# make all rows sum to 1\n",
    "wei = wei / torch.sum(wei, 1, keepdim = True) # (T, T)\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) ----broadcasting----> (B, T, T) @ (B, T, C) âž¡ï¸ (B, T, C)\n",
    "\n",
    "# check if xbow2 is the same as xbow\n",
    "print(torch.allclose(xbow, xbow2, atol = 1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 3: Introducing Softmax in Matrix Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enhance our matrix manipulation by applying softmax to a lower triangular matrix, modified to replace upper values with negative infinity. This focuses the model on prior tokens by normalizing weights into a probability distribution. When multiplied with the data matrix, this method yields a contextually weighted sequence. The softmax transformation ensures that each token's influence is adjusted dynamically, improving the representational accuracy of sequential data. The effectiveness of this approach is verified using torch.allclose, comparing the new outputs to established benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei:\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "wei:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "# we start with zeros, but later these will be replaced with data dependent values (affinities)\n",
    "wei = torch.zeros((T, T))\n",
    "# masked_fill: for all elements where tril == 0, replace with float(\"-inf\")\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "print(f\"wei:\\n{wei}\")\n",
    "wei = F.softmax(wei, dim = -1)\n",
    "print(f\"wei:\\n{wei}\")\n",
    "xbow3 = wei @ x\n",
    "\n",
    "# check if xbow3 is the same as xbow\n",
    "print(torch.allclose(xbow, xbow3, atol = 1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refining the Bigram Language Model with Variable Embedding Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our latest update to the Bigram Language Model, we introduce a variable embedding size, allowing for greater flexibility in handling different dimensions of token representations. By removing the vocab_size parameter from the constructor and defining it globally, we streamline the model's interface. The embedding layer will be adjusted to output a size of n_embed rather than the fixed vocabulary size. To connect these embeddings to our desired output dimension, a linear layer that outputs to the vocab_size will have to be added. This structure aims to enhances the model's ability to learn more nuanced features from the data while maintaining direct compatibility with the vocabulary size through the final output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    # no need to pass vocab_size as an argument, since it is a global variable in this file\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a loockup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        # the output layer is a linear layer with vocab_size outputs\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # idx and targets are both (B, T) tensor of ints\n",
    "        token_emb = self.token_embedding_table(idx) # (B, T, C) = (4, 8 , vocab_size)\n",
    "        logits = self.lm_head(token_emb) # (B, T, vocab_size) = (4, 8, vocab_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Incorporating Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further enhance the model, we can integrate a positional encoding layer. This addition assigns a unique embedding to each position in a sequence, ensuring that the model recognizes not only the content of tokens but also their specific positions within sequences. The positional embeddings are summed with the token embeddings, giving the input representation spatial information before it is processed by the subsequent layers. This method is particularly effective in maintaining the sequential integrity of the data, providing the model with essential temporal cues that significantly improve the accuracy and relevance of its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    # no need to pass vocab_size as an argument, since it is a global variable in this file\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a loockup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        # each position is also associated with an embedding vector\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        # the output layer is a linear layer with vocab_size outputs\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B, T) tensor of ints\n",
    "        token_emb = self.token_embedding_table(idx) # (B, T, C) = (4, 8 , vocab_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = idx.device)) # (T, C) = (8, vocab_size)\n",
    "        # x has the token identities + the position embeddings\n",
    "        x = token_emb + pos_emb # (B, T, C) = (4, 8, vocab_size)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size) = (4, 8, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 4: Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the self-attention framework, each token (also referred to as a node) generates three distinct vectors to facilitate a nuanced interaction within the sequence:\n",
    "\n",
    "* Query: Represents what the token is actively seeking within the sequence.\n",
    "* Key: Describes the unique characteristics that the token possesses.\n",
    "* Value: Contains the actual information or message the token can impart if selected.\n",
    "\n",
    "The mechanism begins by calculating affinities, or weights (wei), by taking the dot product of a token's Query vector with all Key vectors across the sequence. A high alignment between a Query and a Key results in a high affinity value, indicating a strong relevance between the tokens, thus highlighting segments of the sequence that are particularly significant for learning.\n",
    "\n",
    "Rather than directly applying these weights to the original tokens, they are used to scale the Value vectors. This approach ensures that the model emphasizes and learns from the most contextually relevant information contained within the sequence. This method is key to enhancing the model's ability to understand and generate sequences based on the internal dynamics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei[0]: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "\n",
    "# x is private information of each token\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias = False)\n",
    "query = nn.Linear(C, head_size, bias = False)\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "\n",
    "k = key(x) # (B, T, head_size) = (4, 8, 16)\n",
    "q = query(x) # (B, T, head_size) = (4, 8, 16)\n",
    "\n",
    "# now every token in every batch is associated with a key and a query (in parallel), no communication between tokens has happened yet\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei are no longer zeros, but data dependent values (affinities)\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "print(f\"wei[0]: {wei[0]}\")\n",
    "\n",
    "# multiply with value instead of x\n",
    "v = value(x) # (B, T, head_size) = (4, 8, 16)\n",
    "out = wei @ v # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "# out = wei @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes About Self-Attention\n",
    "\n",
    "- **Mechanism Overview**:\n",
    "  - Self-attention acts as a mechanism where nodes (tokens) in a directed graph observe and aggregate information from each other.\n",
    "  - It aggregates this information as a weighted sum, with weights determined by the data-dependent relevance of each node.\n",
    "\n",
    "- **Spatial Independence**:\n",
    "  - Self-attention processes a set of vectors without inherent spatial relationships, hence the necessity for positional encoding to provide context about the sequence order.\n",
    "\n",
    "- **Batch Processing**:\n",
    "  - Each example within a batch is processed independently, ensuring no cross-communication between different sequence examples in the batch.\n",
    "\n",
    "- **Encoder vs. Decoder Attention Blocks**:\n",
    "  - **Encoder Attention**: Allows all tokens to interact freely; commonly used in applications like translation and sentiment analysis.\n",
    "  - **Decoder Attention**: Uses triangular masking to restrict attention flow, ensuring that each token can only attend to previous tokens (autoregressive property). This is typical in tasks like language modeling.\n",
    "\n",
    "- **Self-Attention vs. Cross-Attention**:\n",
    "  - **Self-Attention**: Both keys and values are derived from the same input as the queries.\n",
    "  - **Cross-Attention**: While queries are derived from one source (e.g., the decoder), keys and values are sourced from another (e.g., the encoder).\n",
    "\n",
    "- **Scaled Dot-Product Attention**:\n",
    "  - Involves scaling the attention weights (`wei`) by the square root of the dimensionality of the key vectors to maintain unit variance. This prevents softmax from becoming too peaky at initialization, ensuring a more uniform distribution of attention weights and avoiding extreme focus on a single vector unless warranted by high relevance.\n",
    "\n",
    "These mechanisms ensure that self-attention models can adaptively highlight and integrate relevant information from sequences based on the internal dynamics of the data, making it highly effective for a variety of complex sequence modeling tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unscaled Attention\n",
      "var(k) = 1.044861912727356\n",
      "var(q) = 1.0700464248657227\n",
      "var(wei) = 17.46897315979004\n",
      "\n",
      "Scaled Attention\n",
      "var(k) = 1.044861912727356\n",
      "var(q) = 1.0700464248657227\n",
      "var(wei) = 1.0918108224868774\n"
     ]
    }
   ],
   "source": [
    "# Scaled Attention\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "\n",
    "print(\"Unscaled Attention\")\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "print(f\"var(k) = {torch.var(k)}\")\n",
    "print(f\"var(q) = {torch.var(q)}\")\n",
    "print(f\"var(wei) = {torch.var(wei)}\")\n",
    "\n",
    "print(\"\\nScaled Attention\")\n",
    "wei = q @ k.transpose(-2, -1) * (head_size ** -0.5)\n",
    "print(f\"var(k) = {torch.var(k)}\")\n",
    "print(f\"var(q) = {torch.var(q)}\")\n",
    "print(f\"var(wei) = {torch.var(wei)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first add a single attention head and then once we are done, we will move on to adding several of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding a single Self Attention Head to the Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Head Class\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
    "        # since tril isn't a parameter, we register it as a buffer\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "\n",
    "        # compute attention scores (affinities)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5) # (B, T, C) @ (B, C, T) = (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0 , float(\"-inf\")) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim = -1) # (B, T, T)\n",
    "\n",
    "        # perform weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can modify the bigram language model. we will be adding \"Head\" to the BigramLanguageModel class and forward pass, as well as cropping idx for \"generate\" function. This cropping is done in order to keep idx.shape <= block_size, as we are using positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Head to the BigramLanguageModel\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    # no need to pass vocab_size as an argument, since it is a global variable in this file\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a loockup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        # each position is also associated with an embedding vector\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        # a single head of self attention\n",
    "        self.sa_head = Head(n_embed)\n",
    "        # the output layer is a linear layer with vocab_size outputs\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B, T) tensor of ints\n",
    "        token_emb = self.token_embedding_table(idx) # (B, T, C) = (4, 8 , vocab_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = idx.device)) # (T, C) = (8, vocab_size)\n",
    "        # x has the token identities + the position embeddings\n",
    "        x = token_emb + pos_emb # (B, T, C) = (4, 8, vocab_size)\n",
    "        # feed the input to the self attention head\n",
    "        x = self.sa_head(x) # (B, T, C) = (4, 8, vocab_size)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size) = (4, 8, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # note that F.cross_entropy accepts inputs in shape (B, C, T)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T) # can be as targets = targets.view(-1)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:] # (B, T)\n",
    "            # get the logits for the next token\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            # (note that we are feeding the whole context each time, however we only care about the last prediction)\n",
    "            # (this make doesn't make sense now, but the function will be modified later)\n",
    "            logits = logits[:, -1, :] # Becomes (B, C) (get the last time step for each sequence)\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim = -1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled token to the context\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now multi-headed attention is much easier to implement, the bulk of the work is done. we will add a new class \"MultiHeadAttention\", which will summon \"head\"-s inside it. \n",
    "\n",
    "Before:\n",
    "\n",
    "self.sa_head = Head(n_embed)\n",
    "\n",
    "After:\n",
    "\n",
    "self.sa_heads = MultiHeadAttention(num_head = 4, head_size = n_embed // 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # concatenate them into the channel dimension\n",
    "        return torch.cat([h(x) for h in self.heads], dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's also implement feedforward layer while at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct Transformer blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Block: Communication followed by Computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        \"\"\" n_embed: embedding dimension\n",
    "            n_head: number of heads in the multi-head attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final grooming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Block: Communication followed by Computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        \"\"\" n_embed: embedding dimension\n",
    "            n_head: number of heads in the multi-head attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # residual connection (add the input to the output)\n",
    "        x = x + self.sa(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "    \n",
    "# Multi Head Attention Class\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
    "        # linear transformation to the output of the multi-head attention as projection back to the residual pathway\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # out is the outptu of the multi-head attention\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        # apply a linear layer to the concatenated output\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "# Feed Forward Class\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # multiply by 4 to follow the original implementation\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * 4, n_embed),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of first column: 0.1469 | std of first column: 0.8803\n",
      "mean of first row: -0.0000 | std of first row: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# after normalizing the rows (and removing the buffers too)\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum = 0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim= True)\n",
    "        xvar = x.var(1, keepdim= True)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = BatchNorm1d(100)\n",
    "x = torch.randn(32, 100)\n",
    "x = module(x)\n",
    "x.shape\n",
    "\n",
    "# columns are not normalized now\n",
    "print(f\"mean of first column: {x[:, 0].mean():.4f} | std of first column: {x[:, 0].std():.4f}\")\n",
    "# rows are normalized now\n",
    "print(f\"mean of first row: {x[0, :].mean():.4f} | std of first row: {x[0, :].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Block: Communication followed by Computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        \"\"\" n_embed: embedding dimension\n",
    "            n_head: number of heads in the multi-head attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        # ln1 is applied directly on input before the multi-head attention\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        # ln2 is applied directly on the output of the multi-head attention before the feed-forward layer\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # residual connection (add the input to the output)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class BigramLanguageModel(nn.Module):\n",
    "    # no need to pass vocab_size as an argument, since it is a global variable in this file\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a loockup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        # each position is also associated with an embedding vector\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "                Block(n_embed, n_head = 4),\n",
    "                Block(n_embed, n_head = 4),\n",
    "                Block(n_embed, n_head = 4),\n",
    "                # add layernorm here\n",
    "                nn.LayerNorm(n_embed),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we scale up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    # no need to pass vocab_size as an argument, since it is a global variable in this file\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a loockup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        # each position is also associated with an embedding vector\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head = 4) for _ in range(n_layer)])\n",
    "        # Remember to add it in forward too\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python bigram.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
